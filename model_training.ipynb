{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "class MappingNetwork(keras.Model):\n",
    "    def __init__(self, latent_dim, style_dim, num_layers):\n",
    "        super(MappingNetwork, self).__init__()\n",
    "        self.network = keras.Sequential([\n",
    "            keras.layers.Dense(style_dim, activation='relu')\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "    \n",
    "    def call(self, z):\n",
    "        return self.network(z)\n",
    "\n",
    "class AdaIN(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(AdaIN, self).__init__()\n",
    "    \n",
    "    def call(self, x, style):\n",
    "        mean = tf.reduce_mean(x, axis=[1, 2], keepdims=True)\n",
    "        std = tf.math.reduce_std(x, axis=[1, 2], keepdims=True) + 1e-8\n",
    "        y = (x - mean) / std\n",
    "        \n",
    "        style = tf.reshape(style, [-1, 1, 1, style.shape[-1]])\n",
    "        return y * style[:, :, :, :x.shape[-1]] + style[:, :, :, x.shape[-1]:]\n",
    "\n",
    "class StyleLayer(keras.layers.Layer):\n",
    "    def __init__(self, filters, kernel_size, upsample=False):\n",
    "        super(StyleLayer, self).__init__()\n",
    "        self.upsample = upsample\n",
    "        self.conv = keras.layers.Conv2D(filters, kernel_size, padding='same')\n",
    "        self.adain = AdaIN()\n",
    "        self.activation = keras.layers.LeakyReLU(0.2)\n",
    "    \n",
    "    def call(self, x, style):\n",
    "        if self.upsample:\n",
    "            x = tf.image.resize(x, (x.shape[1]*2, x.shape[2]*2), method='bilinear')\n",
    "        x = self.conv(x)\n",
    "        x = self.adain(x, style)\n",
    "        return self.activation(x)\n",
    "\n",
    "class Generator(keras.Model):\n",
    "    def __init__(self, latent_dim, style_dim, num_layers, channels):\n",
    "        super(Generator, self).__init__()\n",
    "        self.mapping = MappingNetwork(latent_dim, style_dim, num_layers)\n",
    "        self.initial_const = tf.Variable(tf.random.normal([1, 4, 4, channels[0]]))\n",
    "        self.style_layers = [\n",
    "            StyleLayer(ch, 3, upsample=True) for ch in channels[1:]\n",
    "        ]\n",
    "        self.to_rgb = keras.layers.Conv2D(3, 1, activation='tanh')\n",
    "    \n",
    "    def call(self, z):\n",
    "        w = self.mapping(z)\n",
    "        x = tf.tile(self.initial_const, [tf.shape(z)[0], 1, 1, 1])\n",
    "        \n",
    "        for layer in self.style_layers:\n",
    "            x = layer(x, w)\n",
    "        \n",
    "        return self.to_rgb(x)\n",
    "\n",
    "class Discriminator(keras.Model):\n",
    "    def __init__(self, channels):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.layers_list = [\n",
    "            keras.layers.Conv2D(ch, 3, strides=2, padding='same', activation='leaky_relu')\n",
    "            for ch in reversed(channels)\n",
    "        ]\n",
    "        self.flatten = keras.layers.Flatten()\n",
    "        self.fc = keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, x):\n",
    "        for layer in self.layers_list:\n",
    "            x = layer(x)\n",
    "        x = self.flatten(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "class StyleGAN(keras.Model):\n",
    "    def __init__(self, latent_dim=512, style_dim=512, num_layers=8, channels=[512, 256, 128, 64]):\n",
    "        super(StyleGAN, self).__init__()\n",
    "        self.generator = Generator(latent_dim, style_dim, num_layers, channels)\n",
    "        self.discriminator = Discriminator(channels)\n",
    "        self.latent_dim = latent_dim\n",
    "    \n",
    "    def compile(self, g_optimizer, d_optimizer, loss_fn):\n",
    "        super(StyleGAN, self).compile()\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "    \n",
    "    def train_step(self, real_images):\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        latent_vectors = tf.random.normal([batch_size, self.latent_dim])\n",
    "        \n",
    "        with tf.GradientTape() as g_tape, tf.GradientTape() as d_tape:\n",
    "            fake_images = self.generator(latent_vectors)\n",
    "            \n",
    "            real_output = self.discriminator(real_images)\n",
    "            fake_output = self.discriminator(fake_images)\n",
    "            \n",
    "            g_loss = self.loss_fn(tf.ones_like(fake_output), fake_output)\n",
    "            d_loss = self.loss_fn(tf.ones_like(real_output), real_output) + \\\n",
    "                     self.loss_fn(tf.zeros_like(fake_output), fake_output)\n",
    "        \n",
    "        g_gradients = g_tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        d_gradients = d_tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "        \n",
    "        self.g_optimizer.apply_gradients(zip(g_gradients, self.generator.trainable_variables))\n",
    "        self.d_optimizer.apply_gradients(zip(d_gradients, self.discriminator.trainable_variables))\n",
    "        \n",
    "        return {\"g_loss\": g_loss, \"d_loss\": d_loss}\n",
    "\n",
    "# Usage example\n",
    "latent_dim = 512\n",
    "style_dim = 512\n",
    "num_layers = 8\n",
    "channels = [512, 256, 128, 64]\n",
    "batch_size = 32\n",
    "\n",
    "model = StyleGAN(latent_dim, style_dim, num_layers, channels)\n",
    "model.compile(\n",
    "    g_optimizer=keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.0, beta_2=0.99, epsilon=1e-8),\n",
    "    d_optimizer=keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.0, beta_2=0.99, epsilon=1e-8),\n",
    "    loss_fn=keras.losses.BinaryCrossentropy(from_logits=True)\n",
    ")\n",
    "\n",
    "# Generate dummy data for demonstration\n",
    "dummy_images = tf.random.normal([batch_size, 64, 64, 3])\n",
    "\n",
    "# Train for one step (in practice, you would train for many steps)\n",
    "model.train_step(dummy_images)\n",
    "\n",
    "print(\"Training step completed\")\n",
    "\n",
    "# Generate a sample image\n",
    "sample_latent = tf.random.normal([1, latent_dim])\n",
    "generated_image = model.generator(sample_latent)\n",
    "print(\"Sample image generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "# Define the generator network\n",
    "def make_generator():\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(128, input_shape=(100,)),\n",
    "        keras.layers.LeakyReLU(alpha=0.2),\n",
    "        keras.layers.ReShape\n",
    "        keras.layers.Dense(256),\n",
    "        keras.layers.LeakyReLU(alpha=0.2),\n",
    "        keras.layers.Dense(512),\n",
    "        keras.layers.LeakyReLU(alpha=0.2),\n",
    "        keras.layers.Dense(784, activation='tanh')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Define the critic network\n",
    "def make_critic():\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(512, input_shape=(784,)),\n",
    "        keras.layers.LeakyReLU(alpha=0.2),\n",
    "        keras.layers.Dense(256),\n",
    "        keras.layers.LeakyReLU(alpha=0.2),\n",
    "        keras.layers.Dense(128),\n",
    "        keras.layers.LeakyReLU(alpha=0.2),\n",
    "        keras.layers.Dense(1)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Define the WGAN model\n",
    "class WGAN(keras.Model):\n",
    "    def __init__(self, critic, generator, latent_dim, critic_extra_steps=5):\n",
    "        super(WGAN, self).__init__()\n",
    "        self.critic = critic\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.c_extra_steps = critic_extra_steps\n",
    "\n",
    "    def compile(self, c_optimizer, g_optimizer, c_loss_fn, g_loss_fn):\n",
    "        super(WGAN, self).compile()\n",
    "        self.c_optimizer = c_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.c_loss_fn = c_loss_fn\n",
    "        self.g_loss_fn = g_loss_fn\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, real_images):\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "\n",
    "        for _ in range(self.c_extra_steps):\n",
    "            random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "            with tf.GradientTape() as tape:\n",
    "                fake_images = self.generator(random_latent_vectors)\n",
    "                fake_logits = self.critic(fake_images)\n",
    "                real_logits = self.critic(real_images)\n",
    "\n",
    "                c_loss = self.c_loss_fn(real_logits, fake_logits)\n",
    "                \n",
    "            c_gradient = tape.gradient(c_loss, self.critic.trainable_variables)\n",
    "            self.c_optimizer.apply_gradients(zip(c_gradient, self.critic.trainable_variables))\n",
    "\n",
    "            # Clip critic weights\n",
    "            for w in self.critic.trainable_weights:\n",
    "                w.assign(tf.clip_by_value(w, -0.01, 0.01))\n",
    "\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        with tf.GradientTape() as tape:\n",
    "            generated_images = self.generator(random_latent_vectors)\n",
    "            generated_logits = self.critic(generated_images)\n",
    "            g_loss = self.g_loss_fn(generated_logits)\n",
    "\n",
    "        gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        self.g_optimizer.apply_gradients(zip(gen_gradient, self.generator.trainable_variables))\n",
    "\n",
    "        return {\"c_loss\": c_loss, \"g_loss\": g_loss}\n",
    "\n",
    "# Loss functions\n",
    "def critic_loss(real_logits, fake_logits):\n",
    "    return tf.reduce_mean(fake_logits) - tf.reduce_mean(real_logits)\n",
    "\n",
    "def generator_loss(fake_logits):\n",
    "    return -tf.reduce_mean(fake_logits)\n",
    "\n",
    "# Create and compile the WGAN model\n",
    "latent_dim = 100\n",
    "generator = make_generator()\n",
    "critic = make_critic()\n",
    "wgan = WGAN(critic=critic, generator=generator, latent_dim=latent_dim)\n",
    "\n",
    "c_optimizer = keras.optimizers.RMSprop(learning_rate=0.00005)\n",
    "g_optimizer = keras.optimizers.RMSprop(learning_rate=0.00005)\n",
    "\n",
    "wgan.compile(\n",
    "    c_optimizer=c_optimizer,\n",
    "    g_optimizer=g_optimizer,\n",
    "    c_loss_fn=critic_loss,\n",
    "    g_loss_fn=generator_loss\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "latent_dim = 100\n",
    "generator = create_generator(latent_dim)\n",
    "discriminator = create_discriminator()\n",
    "\n",
    "wgan = WGAN_GP(generator, discriminator, latent_dim)\n",
    "\n",
    "# Define optimizers\n",
    "generator_optimizer = tf.keras.optimizers.Adam(0.0001, beta_1=0, beta_2=0.9)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(0.0001, beta_1=0, beta_2=0.9)\n",
    "\n",
    "# Define loss functions\n",
    "def discriminator_loss(real_logits, fake_logits):\n",
    "    return tf.reduce_mean(fake_logits) - tf.reduce_mean(real_logits)\n",
    "\n",
    "def generator_loss(fake_logits):\n",
    "    return -tf.reduce_mean(fake_logits)\n",
    "\n",
    "wgan.compile(\n",
    "    d_optimizer=discriminator_optimizer,\n",
    "    g_optimizer=generator_optimizer,\n",
    "    d_loss_fn=discriminator_loss,\n",
    "    g_loss_fn=generator_loss,\n",
    ")\n",
    "\n",
    "# Train the model (you'll need to prepare your dataset)\n",
    "# wgan.fit(dataset, epochs=num_epochs)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
